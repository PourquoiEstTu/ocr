\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{hyperref} %new
\usepackage{float}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 146 Project Report:\\textOCR}


\author{Dhruv Chand, Roshaan Quyum, Arin Khandelwal \\
  \texttt{\{chandd9,quyumr,khanda3\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}
\begin{itemize} 
\item We are creating an application that extracts text
from an image, otherwise known as Optical Character
Recognition (OCR). It is a multiclass classification
task using images with text on it as its input
data. It can also be seen as a single label classification
task where each data point will be classified
into only one class representing the character that
it is. We will create a command line application
that takes an image with English text as input and
outputs the text on the image into the terminal. It
involves implementing a number of different parts
of the machine learning and model training process
such as image preprocessing, text recognition, and
character recognition. We shall use machine learning
libraries such as OpenCV, NumPy, Scikit-Learn
and PyTorch to implement them. If we have the
time, we would like to extend the OCR to recognize
text in natural photographs (e.g. recognize text
in a photo of a stop sign).
\end{itemize}

\section{Dataset}

\begin{itemize} 
\item from the sheet. check the data set from. refer to the new data set.
\end{itemize}
\hspace{\parindent}We chiefly used two datasets: 
\href{https://www.kaggle.com/datasets/harieh/ocr-dataset}{OCR-Dataset} and 
\href{https://www.kaggle.com/datasets/harieh/ocr-dataset}{Words MNIST}. 

OCR-Dataset consists of the 26 letters of the English alphabet in  
both lower and uppercase along with all 10 digits; across the 62 classes, there 
are roughly 210 000 images in total. The data uses 3475 different fonts from 
publicly available Google Fonts. It is the dataset we trained 
our model on. 

There are a number of preprocessing steps done to the images 
from this dataset. Images are converted to a specific type (\texttt{np.uint8})
to ensure consistency in the following preprocessing steps. Then, 
all white pixels on the edge of the letter are cropped out (the cropping is 
similar to a bounding box being placed around the letter/digit and then all 
pixels outside of it removed from the image). The image is then scaled to 
have dimensions of 64x64, and finally, it is turned into a tensor. Before 
being fed to the model, a corresponding list of labels is generated from the 
provided labels for more convenient access. After this 
preprocessing, the images from this dataset can be inputted to the model. This 
preprocessing here assumes the image contains a single character; however, for 
images with more than one character, extra preprocessing was needed.

The Words MNIST dataset contains about 10 000 images of various words. All 
characters from the alphabet (lower and uppercase), all 10 digits, as well as an
assortment of special characters/punctuation are in the words in the images. The 
images have variable sizes and need to be resized to be uniform. The images 
mostly come from scanned documents and synthetic generation
Data was synthetically generated to have lesser seen characters included in the 
dataset. Some of the images were labelled manually and some of it was 
labelled using tesseract OCR and then manually checked after for errors. Our 
OCR model is built only to handle individual characters; so before running 
the preprocessing from the previous paragraph on it, the characters in each 
word needed to be segmented. We had two different attempts at this and will 
describe both. 

Our first, less successful segmentation, first grey scaled 
the image, applied median blur to remove ``salt and pepper'' noise, 
applied inverse binary threshold using Otsu's method, and dilated the image 
to make the characters larger in the vertical direction. 
We then found the contours of each character and drew bounding boxes around 
them. The dilation was meant to preserve the gap between characters, which 
manifested horizontally, and remove any ``intra-character'' space, such as 
the hole in an ``a'' or the large amount of space in the column a ``u'' takes 
up so that all characters looked roughly like rectangular blobs. Finally, we used 
the dimensions of each bounding box to crop out the 
characters from the original image and return an array of images containing each 
character. One large flaw of this approach is that if the dilation did not 
remove all the space in a character's rough column location, then multiple 
contours would be drawn in that column. This would mean, for example, that 
both the bounding box containing ``a'' as well as the circle 
in ``a'' would be returned as distinct characters. With the variation in font,
this occurred quite often with ``o''s and ``a''s. 

The second, more successful segmentation function implements the Potential 
Segmentation Columns (PCS) method from the paper:
\href{https://www.researchgate.net/publication/257719290_A_New_Character_Segmentation_Approach_for_Off-Line_Cursive_Handwritten_Words}
{A New Character Segmentation Approach for Off-Line Cursive Handwritten Words}.
The same rough process is done as before with the exception of blurring
before thresholding and instead of dilating the image, we use Zhang 
Suen's method to thin the characters. After thinning, all columns of pixels 
in the images are scanned. If there is less than or exactly 1 pixel (this was 
generally the PCS threshold/limit for the images in the Words MNIST dataset)
of white (meaning less than 1 pixel of character), then the column is marked 
as being a PCS. After the image is fully scanned, the array of marked 
segmentation columns is looped through. For each set of consecutive PCSs, 
the middle column of the set is chosen as the segmentation column and the 
rest are unmarked. Finally, the original image is cropped according to these 
columns and the individual images are returned. This method worked much better
than the first one and is near guaranteed to work when there are gaps between
characters. When there aren't any gaps, it still performs well and better than
the first method, but clearly not as good. This method was chosen to character 
segmentation due to its consistency (it is easier to find out why it fails)
and better quality.

\section{Features and Inputs}

\begin{itemize} 
\item diff features and inputs for pre processing. 
\end{itemize}

\hspace{\parindent}The inputs to the current model are 64x64 dimension 
images. In our final version of our model, no feature engineering, 
representation learning, feature selection, or augmentation was done. The 
preprocessing as described above was done and then the image(s) were fed 
into the model. However, before we finalized our model, we used a different 
set of features. 

\section{Implementation}


% Our initial model was implemented using Pytorch using a Convolutional Neural Network

\section{Evaluation and Progress}
\begin{itemize} 
\item Copy the description of the old model. 
\end{itemize}
\section{Error Analysis}

% \section{chat}
\begin{itemize} 
\item Need to figure out
\end{itemize}

% \section{Template Notes}

% You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.

% \subsection{Tables and figures}

% See Table~\ref{citation-guide} for an example of a table and its caption.
% See Figure~\ref{fig:experiments} for an example of a figure and its caption.


% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}


% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% \subsection{References} 

% Many websites where you can find academic papers also allow you to export a bib file for citation or bib formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the \LaTeX{}. You can remove the example entries.

% Leave these in they're required

\nocite{deCampos09}

\nocite{abdur_rahim_mia_abdullah_al_mamun_abdullah_al_sajid_aurunave_mollik_ruddra_2024}

\nocite{DBLP:journals/corr/abs-2105-05486}

\nocite{DBLP:journals/corr/abs-1904-08920}

\nocite{westerdijk2025improvingocrhistoricaltexts}

\nocite{kiessling:hal-04936936}

\nocite{DBLP:journals/corr/abs-2109-10282}

\nocite{DBLP:journals/corr/abs-1802-02611}

\nocite{OCR-CRNN}

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.
% This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like this: $A=\pi r^2$.


% % \section*{Limitations}

\section*{Team Contributions}

% Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7. \\


% \bibliography{custom}



\section{Figures and Tables}

\clearpage

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
